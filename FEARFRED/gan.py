# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_GAN_Embedder.ipynb.

# %% auto 0
__all__ = ['FRED_A_GAN']

# %% ../nbs/04_GAN_Embedder.ipynb 2
import torch
import torch.nn as nn
import torch.nn.functional as F
from .generator import *
from .discriminator import *
from .graph_builder import *
class FRED_A_GAN(nn.Module):
    def __init__(self,
                intrinsic_dimension, # intrinsic dimension of data
                n_nodes, # number of nodes in directed subgraphs
                n_features, # number of features per node
                ):
        super().__init__()
        self.intrinsic_dimension = intrinsic_dimension
        self.n_nodes = n_nodes
        self.n_features = n_features + 1 # we include an extra vector of 1s when feeding to the discriminator
        # initialize GAN machineru
        self.generator = FlowGenerator(self.intrinsic_dimension,n_features) # generates the actual number of featuers
        self.discriminator = ScatteringDiscriminator(self.n_nodes,self.n_features)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.has_mps else "cpu")
        
    def generate_fake(self):
        # 1. sample from the unit hypercube
        # TODO: Could adjust this to sample from parts of the hypercube
        # to get localized subgraphs
        samples = torch.randn(self.n_nodes,self.intrinsic_dimension, device = self.device)
        # 2. Translate to a sample in the embedding space, and take flows
        points, flows, features = self.generator(samples)
        # 3. construct a directed graph based off of these points and flows, 
        # and create summary node features for it
        A = flashlight_kernel(points,flows,kernel_type='fixed', sigma=0.7)
        # simplify graph with this non-linearity
        A[A<0.01] = 0
        # TODO: Might have to revise that.
        node_features = torch.ones(self.n_nodes,self.n_features, device=self.device).float()
        node_features[:,1:] = features
        return A, node_features
    
    def train_critic(self, A, features):
        self.device = A.device
        # generate a fake image and run it through the discriminator
        fakeA, fake_features = self.generate_fake()
        # detach gradients when training critic, to prevent unnecessary backprop graph construction
        fakeA = fakeA.detach()
        fake_features = fake_features.detach()
        witness_of_fake = self.discriminator(fakeA,fake_features)
        
        # Test the critic on real data
        node_features = torch.ones(self.n_nodes,self.n_features, device=self.device).float()
        node_features[:,1:] = features
        # Run through discriminator and compute loss
        witness_of_real = self.discriminator(A,node_features)
        # Loss is the difference between the witness function of fake and real
        # The critic wants to maximize this difference
        loss = witness_of_fake - witness_of_real
        return loss
        
    def train_generator(self,A,features):
        # 1. sample from the unit hypercube
        # TODO: Could adjust this to sample from parts of the hypercube
        # set device to match input device
        self.device = A.device
        # to get localized subgraphs
        fakeA, fake_features = self.generate_fake()
        # 4. Run the graph and its features through the discriminator
        witness_of_fake = self.discriminator(fakeA,fake_features)
        # generator wants to minimize the witness function on its data
        loss = - witness_of_fake
        return loss
        
