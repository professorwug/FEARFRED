[
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  },
  {
    "objectID": "manifolds.html",
    "href": "manifolds.html",
    "title": "00c Manifold Datasets",
    "section": "",
    "text": "We first introduce two useful helper functions. The first function tilts the 1-manifold in 2d plane into 3d space. The second function adds noise to data positions.\n\nsource\n\n\n\n xy_tilt (X, flows, xtilt=0, ytilt=0)\n\n\nsource\n\n\n\n\n rotation_transform (X:numpy.ndarray, tilt_angles)\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nX\nndarray\nThe input matrix, of size n x d (d is # dimensions)\n\n\ntilt_angles\n\na list of d-1 values in [0,2pi] specifying how much to tilt in d-1 the xy, yz (…) planes\n\n\n\n\nsource\n\n\n\n\n add_noise (X, sigma=0)\n\n\n\n\nWe sample \\(\\theta\\) uniformly between \\(0\\) and \\(2\\pi\\) and calculate \\(x = r\\cos(\\theta), y = r\\sin(\\theta)\\). The vector at each data point is tangent to the circle.\n/Users/adjourner/miniforge3/envs/FEARFRED/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: potentially wrong underline length... \nParameters \n----------- in \nSample `n` data points on a circle.\nIn addition to the points, returns a \"flow\" vector at each point. ...\n  else: warn(msg)\n\nsource\n\n\n\n directed_circle (num_nodes=500, radius=1, xtilt=0, ytilt=0, sigma=0,\n                  inverse=False)\n\nSample n data points on a circle. In addition to the points, returns a “flow” vector at each point.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_nodes\nint\n500\nNumber of data points in shape.\n\n\nradius\nint\n1\nRadius of circle.\n\n\nxtilt\nint\n0\nAngle to rotate around the x-axis.\n\n\nytilt\nint\n0\nAngle to rotate around the y-axis.\n\n\nsigma\nint\n0\nAmount of gaussian noise\n\n\ninverse\nbool\nFalse\nWhether to flip the direction of flow\n\n\n\n\n# collapse\nX, flows, labels = directed_circle()\nplot_directed_2d(X, flows, labels)\n\n\n\n\n\n# collapse\nX, flows, labels = directed_circle(xtilt=np.pi/4, sigma=0.1)\nplot_directed_3d(X, flows, labels, origin=True)\n\n\n\n\n\n\n\n\nWe sample \\(\\theta\\) uniformly between \\(0\\) and \\(2\\pi \\times\\) num_spirals and calculate \\(x = r\\theta\\cos(\\theta), y = r\\theta\\sin(\\theta)\\). The vector at each data point is tangent to the spiral and its length is proportional to \\(\\theta\\).\n/Users/adjourner/miniforge3/envs/FEARFRED/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: potentially wrong underline length... \nParameters \n----------- in \nSample `n` data points on a spiral.\nIn addition to the points, returns a \"flow\" vector at each point. ...\n  else: warn(msg)\n\nsource\n\n\n\n directed_spiral (num_nodes=500, num_spirals=1.5, radius=1, xtilt=0,\n                  ytilt=0, sigma=0, inverse=False)\n\nSample n data points on a spiral. In addition to the points, returns a “flow” vector at each point.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_nodes\nint\n500\nNumber of data points in shape.\n\n\nnum_spirals\nfloat\n1.5\nNumber of revolution.\n\n\nradius\nint\n1\nRadius of spiral.\n\n\nxtilt\nint\n0\nAngle to rotate around the x-axis.\n\n\nytilt\nint\n0\nAngle to rotate around the y-axis.\n\n\nsigma\nint\n0\nAmount of gaussian noise\n\n\ninverse\nbool\nFalse\nWhether to flip the direction of flow\n\n\n\n\nX, flows, labels = directed_spiral()\nplot_directed_2d(X, flows, labels)\n\n\n\n\n\nX, flows, labels = directed_spiral(xtilt=np.pi/4, sigma=0.2)\nplot_directed_3d(X, flows, labels, origin=True)\n\n\n\n\n\n\n\n\nWe sample \\(\\theta\\) between \\(0\\) and \\(2\\pi \\times\\) num_spirals such that the probability of choosing \\(\\theta_0\\) is proportional to the value of \\(\\theta_0\\). We calculate \\(x = r\\cos(\\theta), y = r\\sin(\\theta)\\). The vector at each data point is tangent to the spiral and is unit length.\n\nsource\n\n\n\n directed_spiral_uniform (num_nodes=500, num_spirals=1.5, radius=1,\n                          xtilt=0, ytilt=0, sigma=0, inverse=False)\n\nSample n data points on a spiral. In addition to the points, returns a “flow” vector at each point.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_nodes\nint\n500\nNumber of data points in shape.\n\n\nnum_spirals\nfloat\n1.5\nNumber of revolution.\n\n\nradius\nint\n1\nRadius of spiral.\n\n\nxtilt\nint\n0\nAngle to rotate around the x-axis.\n\n\nytilt\nint\n0\nAngle to rotate around the y-axis.\n\n\nsigma\nint\n0\nAmount of gaussian noise\n\n\ninverse\nbool\nFalse\nWhether to flip the direction of flow\n\n\n\n\nX, flows, labels = directed_spiral_uniform()\nplot_directed_2d(X, flows, labels)\n\n\n\n\n\nX, flows, labels = directed_spiral_uniform(xtilt=np.pi/4, sigma=0.2)\nplot_directed_3d(X, flows, labels, origin=True)\n\n\n\n\n\n\n\n\nWe sample \\(\\theta\\) uniformly between \\(\\pi \\times\\) num_spirals and \\(3\\pi \\times\\) num_spirals and calculate \\(x = r\\theta\\cos(\\theta), y = r\\theta\\sin(\\theta)\\). The vector at each data point is tangent to the spiral and its length is proportional to \\(\\theta\\).\n\nsource\n\n\n\n directed_spiral_delayed (num_nodes=500, num_spirals=1.5, radius=1,\n                          xtilt=0, ytilt=0, sigma=0, inverse=False)\n\nSample n data points on a spiral. In addition to the points, returns a “flow” vector at each point.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_nodes\nint\n500\nNumber of data points in shape.\n\n\nnum_spirals\nfloat\n1.5\nNumber of revolution.\n\n\nradius\nint\n1\nRadius of spiral.\n\n\nxtilt\nint\n0\nAngle to rotate around the x-axis.\n\n\nytilt\nint\n0\nAngle to rotate around the y-axis.\n\n\nsigma\nint\n0\nAmount of gaussian noise\n\n\ninverse\nbool\nFalse\nWhether to flip the direction of flow\n\n\n\n\nX, flows, labels = directed_spiral_delayed()\nplot_directed_2d(X, flows, labels)\n\n\n\n\n\n# collapse\nX, flows, labels = directed_spiral_delayed(xtilt=np.pi/4, sigma=0.2)\nplot_directed_3d(X, flows, labels, origin=True)"
  },
  {
    "objectID": "manifolds.html#cylinders",
    "href": "manifolds.html#cylinders",
    "title": "00c Manifold Datasets",
    "section": "Cylinders",
    "text": "Cylinders\nWe simply generate a prism from directed_circle.\n/Users/adjourner/miniforge3/envs/FEARFRED/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: potentially wrong underline length... \nParameters \n----------- in \nSample `n` data points on a cylinder.\nIn addition to the points, returns a \"flow\" vector at each point. ...\n  else: warn(msg)\n\nsource\n\ndirected_cylinder\n\n directed_cylinder (num_nodes=1000, height=20, radius=1, xtilt=0, ytilt=0,\n                    sigma=0, inverse=False)\n\nSample n data points on a cylinder. In addition to the points, returns a “flow” vector at each point.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_nodes\nint\n1000\nNumber of data points in shape.\n\n\nheight\nint\n20\nHeight of cylinder.\n\n\nradius\nint\n1\nRadius of cylinder.\n\n\nxtilt\nint\n0\nAngle to rotate around the x-axis.\n\n\nytilt\nint\n0\nAngle to rotate around the y-axis.\n\n\nsigma\nint\n0\nAmount of gaussian noise\n\n\ninverse\nbool\nFalse\nWhether to flip the direction of flow\n\n\n\n\nX, flows, labels = directed_cylinder(xtilt=np.pi/4, sigma=0.1)\nplot_directed_3d(X, flows, labels)"
  },
  {
    "objectID": "manifolds.html#naive-swiss-rolls",
    "href": "manifolds.html#naive-swiss-rolls",
    "title": "00c Manifold Datasets",
    "section": "Naive Swiss Rolls",
    "text": "Naive Swiss Rolls\nWe simply generate a prism from directed_spiral.\n/Users/adjourner/miniforge3/envs/FEARFRED/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: potentially wrong underline length... \nParameters \n----------- in \nSample `n` data points on a swiss roll.\nIn addition to the points, returns a \"flow\" vector at each point. ...\n  else: warn(msg)\n\nsource\n\ndirected_swiss_roll\n\n directed_swiss_roll (num_nodes=1000, num_spirals=1.5, height=20,\n                      radius=1, xtilt=0, ytilt=0, sigma=0, inverse=False)\n\nSample n data points on a swiss roll. In addition to the points, returns a “flow” vector at each point.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_nodes\nint\n1000\nNumber of data points in shape.\n\n\nnum_spirals\nfloat\n1.5\nNumber of revolution.\n\n\nheight\nint\n20\nHeight of swiss roll.\n\n\nradius\nint\n1\nRadius of swiss roll.\n\n\nxtilt\nint\n0\nAngle to rotate around the x-axis.\n\n\nytilt\nint\n0\nAngle to rotate around the y-axis.\n\n\nsigma\nint\n0\nAmount of gaussian noise\n\n\ninverse\nbool\nFalse\nWhether to flip the direction of flow\n\n\n\n\n# collapse\nX, flows, labels = directed_swiss_roll(xtilt=np.pi/4, sigma=0.1)\nplot_directed_3d(X, flows, labels)"
  },
  {
    "objectID": "manifolds.html#uniform-spirals-1",
    "href": "manifolds.html#uniform-spirals-1",
    "title": "00c Manifold Datasets",
    "section": "Uniform Spirals",
    "text": "Uniform Spirals\nWe simply generate a prism from directed_spiral_uniform.\n\nsource\n\ndirected_swiss_roll_uniform\n\n directed_swiss_roll_uniform (num_nodes=1000, num_spirals=1.5, height=20,\n                              radius=1, xtilt=0, ytilt=0, sigma=0,\n                              inverse=False)\n\nSample n data points on a swiss roll. In addition to the points, returns a “flow” vector at each point.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_nodes\nint\n1000\nNumber of data points in shape.\n\n\nnum_spirals\nfloat\n1.5\nNumber of revolution.\n\n\nheight\nint\n20\nHeight of swiss roll.\n\n\nradius\nint\n1\nRadius of swiss roll.\n\n\nxtilt\nint\n0\nAngle to rotate around the x-axis.\n\n\nytilt\nint\n0\nAngle to rotate around the y-axis.\n\n\nsigma\nint\n0\nAmount of gaussian noise\n\n\ninverse\nbool\nFalse\nWhether to flip the direction of flow\n\n\n\n\nX, flows, labels = directed_swiss_roll_uniform(xtilt=np.pi/4, sigma=0.1)\nplot_directed_3d(X, flows, labels)"
  },
  {
    "objectID": "manifolds.html#delayed-spirals-1",
    "href": "manifolds.html#delayed-spirals-1",
    "title": "00c Manifold Datasets",
    "section": "Delayed Spirals",
    "text": "Delayed Spirals\nWe simply generate a prism from directed_spiral_delayed.\n\nsource\n\ndirected_swiss_roll_delayed\n\n directed_swiss_roll_delayed (num_nodes=1000, num_spirals=1.5, height=20,\n                              radius=1, xtilt=0, ytilt=0, sigma=0,\n                              inverse=False)\n\nSample n data points on a swiss roll. In addition to the points, returns a “flow” vector at each point.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_nodes\nint\n1000\nNumber of data points in shape.\n\n\nnum_spirals\nfloat\n1.5\nNumber of revolution.\n\n\nheight\nint\n20\nHeight of swiss roll.\n\n\nradius\nint\n1\nRadius of swiss roll.\n\n\nxtilt\nint\n0\nAngle to rotate around the x-axis.\n\n\nytilt\nint\n0\nAngle to rotate around the y-axis.\n\n\nsigma\nint\n0\nAmount of gaussian noise\n\n\ninverse\nbool\nFalse\nWhether to flip the direction of flow\n\n\n\n\nX, flows, labels = directed_swiss_roll_delayed(xtilt=0, sigma=0.1)\nplot_directed_3d(X, flows, labels)"
  },
  {
    "objectID": "visualization_utils.html",
    "href": "visualization_utils.html",
    "title": "Visualization functions",
    "section": "",
    "text": "Plotting 3d Manifolds\n\nsource\n\nplot_directed_3d\n\n plot_directed_3d (X, flow, labels=None, mask_prob=0.5, cmap='viridis',\n                   origin=False, ax=None, save=False, filename='',\n                   plot_minimal=False)\n\n\nsource\n\n\nplot_origin_3d\n\n plot_origin_3d (ax, xlim, ylim, zlim)\n\n\n\n\nPlotting Graphs\n\nsource\n\nvisualize_graph\n\n visualize_graph (adjacency, is_networkx=False, to_undirected=False,\n                  ax=None)"
  },
  {
    "objectID": "gan_embedder.html",
    "href": "gan_embedder.html",
    "title": "04 Directed Graph Embedding by GAN",
    "section": "",
    "text": "Testing that it runs on fake data\n\nA = torch.rand(10,10)\nfeatures = torch.rand(10,2)\nfredtest = FRED_A_GAN(\n    intrinsic_dimension = 2,\n    n_nodes = 10,\n    n_features = 2\n)\n\n\nl_g = fredtest.train_generator(A,features)\n\n\nl_d = fredtest.train_critic(A,features)\n\n\nTraining\n\nn_epochs = 500\nn_critic = 5 # number of times to train the critic for each training iteration of the generator\nweight_clipping_value = 0.01\nfred = FRED_A_GAN(\n    intrinsic_dimension = 2,\n    n_nodes = 128,\n    n_features = 1\n)\nopt_gen = torch.optim.Adam(fred.generator.parameters())\nopt_discrim = torch.optim.Adam(fred.discriminator.parameters())\n\n\nfrom FEARFRED.datasets.manifolds import DirectedCircle\nfrom torch.utils.data import DataLoader\n\n\nd = DirectedCircle()\ndataloader = DataLoader(d, batch_size=1)\n\n\nfrom FEARFRED.plotting import plot_flow_field_2d\nfor e in trange(n_epochs):\n    i = 0\n    for A, features in dataloader:\n        i += 1\n        # shape wrangling: presently each batch has but a single matrix and list of features\n        A = A[0].float()\n        features = features[0][:,None] # reshape to n_nodes x n_features\n        features = features.float()\n        \n        opt_discrim.zero_grad()\n        loss = fred.train_critic(A, features)\n        if loss.item() != loss.item():\n            print(\"A nan has been found in d! investigate please\")\n            raise NotImplementedError\n        loss.backward()\n        opt_discrim.step()\n        # print('discrim loss',loss)\n        \n        for p in fred.discriminator.parameters():\n            p.data.clamp_(-weight_clipping_value, weight_clipping_value)\n\n        if i % n_critic == 0:\n            # Train generator\n            opt_gen.zero_grad()\n            loss = fred.train_generator(A, features)\n            # print('gen loss',loss)\n            if loss.item() != loss.item():\n                print(\"A nan has been found in g! investigate please\")\n                raise NotImplementedError\n            loss.backward()\n            opt_gen.step()\n    if e % 50==0:\n        plot_flow_field_2d(fred.generator)\n\n  0%|                                                    | 0/500 [00:00<?, ?it/s]\n\n\n\n\n\n 10%|████▎                                      | 50/500 [01:39<14:57,  1.99s/it]\n\n\n\n\n\n 20%|████████▍                                 | 100/500 [03:19<13:20,  2.00s/it]\n\n\n\n\n\n 30%|████████████▌                             | 150/500 [04:58<11:27,  1.96s/it]\n\n\n\n\n\n 40%|████████████████▊                         | 200/500 [06:37<09:49,  1.97s/it]\n\n\n\n\n\n 50%|█████████████████████                     | 250/500 [08:15<08:11,  1.96s/it]\n\n\n\n\n\n 60%|█████████████████████████▏                | 300/500 [09:54<06:34,  1.97s/it]\n\n\n\n\n\n 70%|█████████████████████████████▍            | 350/500 [11:33<04:57,  1.98s/it]\n\n\n\n\n\n 80%|█████████████████████████████████▌        | 400/500 [13:12<03:16,  1.96s/it]\n\n\n\n\n\n 90%|█████████████████████████████████████▊    | 450/500 [14:51<01:39,  1.98s/it]\n\n\n\n\n\n100%|██████████████████████████████████████████| 500/500 [16:29<00:00,  1.98s/it]\n\n\nHere are some visualization functions to gauge progress during training\n\nfrom FEARFRED.plotting import plot_flow_field_2d\nplot_flow_field_2d(fred.generator)\n\n\n\n\n\nfred.generate_fake()\n\n(tensor([[0.0000, 0.4712, 0.0761,  ..., 0.0743, 0.0746, 0.0739],\n         [0.0922, 0.0000, 0.0756,  ..., 0.0739, 0.0742, 0.0735],\n         [0.5618, 0.5587, 0.0000,  ..., 0.0754, 0.0757, 0.0749],\n         ...,\n         [0.5492, 0.5459, 0.5567,  ..., 0.0000, 0.3682, 0.0916],\n         [0.5514, 0.5481, 0.5590,  ..., 0.1184, 0.0000, 0.0764],\n         [0.5460, 0.5427, 0.5535,  ..., 0.4742, 0.5643, 0.0000]],\n        grad_fn=<IndexPutBackward0>),\n tensor([[1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1029],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1028],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1028],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1035],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1029],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1038],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1031],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1028],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1029],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027],\n         [1.0000, 3.1027]], grad_fn=<CopySlices>))\n\n\n\nnode_features = torch.ones(len(A),2)\nnode_features[:,1:] = features\n\n\nfred.discriminator(A,node_features)\n\ntensor([[-0.0095]], grad_fn=<AddmmBackward0>)"
  },
  {
    "objectID": "flow_generator.html",
    "href": "flow_generator.html",
    "title": "01 The Flow Generator of Directed Graphs",
    "section": "",
    "text": "FEARFRED is built of three intertwingled components, two standard for a GAN, and one unique to its graph-embedding mission:\nThis notebook defines and tests the Flow Generator.\nsource"
  },
  {
    "objectID": "flow_generator.html#visualizing-the-output",
    "href": "flow_generator.html#visualizing-the-output",
    "title": "01 The Flow Generator of Directed Graphs",
    "section": "Visualizing the output",
    "text": "Visualizing the output\n\n# testing to ensure it runs\nFG = FlowGenerator(dimension=2, n_features=1)\npoints = FG.flow_sampler(torch.rand(50,2))\nflows = FG.flow_artist(points)\n\n\nplot_flow_field_2d(FG)"
  },
  {
    "objectID": "flow_generator.html#can-it-learn-a-circle",
    "href": "flow_generator.html#can-it-learn-a-circle",
    "title": "01 The Flow Generator of Directed Graphs",
    "section": "Can it learn a circle?",
    "text": "Can it learn a circle?\n\nrandom_samples = torch.rand(100,2)*2-1\nangles_of_samples = torch.norm(random_samples,dim=1)*2*torch.pi\nreal_circle, real_flows = torch.zeros_like(random_samples), torch.zeros_like(random_samples)\nreal_circle[:,0] = torch.cos(angles_of_samples)\nreal_circle[:,1] = torch.sin(angles_of_samples)\nreal_flows[:,0] = -torch.sin(angles_of_samples)\nreal_flows[:,1] = torch.cos(angles_of_samples)\nplot_directed_2d(real_circle,real_flows,angles_of_samples)\n\n\n\n\n\ndef circle_loss(points, flows, features, random_samples):\n    loss = 0\n    # calculate angles of points \n    angles_of_samples = torch.norm(random_samples,dim=1)*2*torch.pi\n    real_circle, real_flows = torch.zeros_like(random_samples), torch.zeros_like(random_samples)\n    real_circle[:,0] = torch.cos(angles_of_samples)\n    real_circle[:,1] = torch.sin(angles_of_samples)\n    real_flows[:,0] = -torch.sin(angles_of_samples)\n    real_flows[:,1] = torch.cos(angles_of_samples)\n    # penalize all of the points to have distance 1 from the origin\n    loss += torch.linalg.norm(points - real_circle)\n    # penalize flows to be perpendicular to the points, i.e. have zero cosine similarity\n    loss += torch.sum(1-torch.abs(F.cosine_similarity(flows,real_flows)))\n    # penalize features to be the angles\n    loss += 0.01*torch.linalg.norm(features - angles_of_samples)\n    return loss\n\n\nmodel = FlowGenerator(dimension=2,n_features=1)\ndevice = torch.device('cpu')\nmodel.to(device)\nopt = torch.optim.Adam(model.parameters()) \ntrain_accs, test_accs, losses = [], [], [] # lists to keep track of model stats\nfor e in trange(2000): # this is the number of epochs to train -- each epoch iterates through the entire dataset.\n    model.train()\n    samples = torch.rand(128,2)\n    points, flows, features = model(samples)    \n    l = circle_loss(points,flows,features, samples)\n    l.backward()\n    opt.step()\n    opt.zero_grad()\n    losses.append(l.item())\nplt.plot(losses)\n\n\n\n\n\n\n\n\nplot_flow_field_2d(model)"
  },
  {
    "objectID": "ARCHIVES/eeemd.html",
    "href": "ARCHIVES/eeemd.html",
    "title": "EEEMD: Edge-to-Edge Earth Mover’s Distance",
    "section": "",
    "text": "We want a distance between graphs with the same number of nodes, obeying: 1. The cost of moving an edge from one neighbor to another neighbor is lower than the cost of moving it from a neighbor to a non-neighbor. 2. The distance between a pair of edges having one point in common is smaller than the distance between a pair of edges having no points in common. 3. The above differences in magnitude correspond to the geodesic (manifold) distances on the graph.\nSuch a distance could be useful to FRED. We’ve previously used the KLD between adjacency matrices as a proxy for this graph-to-graph distance, but the KLD doesn’t respect the graph geometry — all edges are equally far apart. As a result, gradients flowing from the KLD can’t give as useful information to the model – insofar as which way to tweak the edges or move the nodes – as might the mythical EEEMD."
  },
  {
    "objectID": "ARCHIVES/eeemd.html#city-block-eeemd",
    "href": "ARCHIVES/eeemd.html#city-block-eeemd",
    "title": "EEEMD: Edge-to-Edge Earth Mover’s Distance",
    "section": "City-block EEEMD",
    "text": "City-block EEEMD\nHere’s one very simple such metric satisfying the above. Let \\(e_{ij}\\) denote the edge between \\(i\\) and \\(j\\). Then we define the distance as:\nNeighboring edges (those which share one node) have distance equal to the transport cost between the nodes that they don’t share:\n\\[d(e_{ij}, e_{ik}) = d_{graph}(j,k)\\]\nWhere \\(d_{graph}\\) is a ground distance between the nodes of \\(G_1\\) at the current stage of surgery.\nBy the triangle inequality, other edges have distance less than or equal to the distance obtained by moving one node to a neighboring node and then moving the other:\n\\[d(e_{ij},e_{kl}) \\leq d(e_{ij}, e_{kj}) + d(e_{kj},e_{kl})\\]\nWe’ll go ahead and declare this a city-block style metric, in which this distance is equal\n\\[d(e_{ij},e_{kl}) = \\text{min}(d(e_{ij}, e_{kj}) + d(e_{kj},e_{jl}), d(e_{ij}, e_{il}) + d(e_{il},e_{kl}))\\]\nThis defines a distance metric between pairs of edges. Using this as a ground distance over the adjacency matrix, we define the City-block EEMD as the EMD between the adjacency matrices under the cityblock edge-to-edge ground distance.\n\nfrom scipy.sparse import csr_array\ndef edge_to_edge_cityblock_distance(\n    A1:torch.Tensor, # Adjacency matrix of first graph (as torch.tensor)\n    A2:torch.Tensor, # Adjacency matrix of second graph\n    node_distances:torch.Tensor # Distance matrix between nodes of graph\n):\n    pass\n\n\nfrom scipy.sparse import csr_array\ndef EEEMD_cityblock(\n    A1, # A torch geometric graph object\n    A2, \n    Dgraph\n):\n    # Calculate edge-to-edge ground distance\n    # Support sparse matrices for more efficient computation: we only care \n    # First, calculate distances between neighbors.\n    # Get nonzero combined indices; we only care about these distances\n    nonzero_indices = np.vstack(csr_array(A1 + A2).nonzero())\n    return nonzero_indices\n\n\n# test case: branch graph\nfrom FEARFRED.datasets.toy_graphs import CycleGraph, HalfCycleGraph\nA1 = CycleGraph(num_nodes=10)[0]\nA2 = HalfCycleGraph(num_nodes=10)[0]\n\n\n# EEEMD_cityblock(A1,A2,)\n\n\nfrom scipy.sparse import csr_array\nU = np.random.rand(10,10)\nUsparse = csr_array(U)\n\n\ntype(A1)\n\ntorch_geometric.data.data.Data"
  },
  {
    "objectID": "scattering_discriminator.html",
    "href": "scattering_discriminator.html",
    "title": "03 Scattering Discriminator",
    "section": "",
    "text": "FEARFRED’s generator creates and samples from a flow field. The Flashlight Kernel converts these samples into a directed graph. At last, we arrive at the final piece in our architecture: a GNN to discriminate between graphs coming from our dataset and graphs coming from the flow field.\nThe trick here is to preserve the chain of differentiability between the generator and discriminator. This makes any translation of the affinity matrix from the flashlight kernel into a PyTorch Geometric format problematic – this translation isn’t differentiable! It’s better to stay as close to the affinity matrix as possible.\nTo do this, we adopt a learnable scattering network, LEGSNet, which operates entirely on the affinity matrix of a graph. This instatiation of LEGSNet uses the scattering transform as our witness function, paired with a simple feedforward classifier to make the final call on in-group membership."
  },
  {
    "objectID": "scattering_discriminator.html#testing",
    "href": "scattering_discriminator.html#testing",
    "title": "03 Scattering Discriminator",
    "section": "Testing",
    "text": "Testing\n\nSD = ScatteringDiscriminator(n_nodes=3,n_features=2)\n\n\nA = torch.rand(3,3)\nx = torch.ones(3,2)\nSD(A,x)\n\ntensor([[-0.0809]], grad_fn=<AddmmBackward0>)"
  },
  {
    "objectID": "graph_builder.html",
    "href": "graph_builder.html",
    "title": "02 Graph Builder",
    "section": "",
    "text": "The Flashlight Affinity Matrix\nFor data in a really low dimension (e.g. 2), we need to incorporate distance into our affinity calculations. We do this pretty simply - by multiplying a standard distance-based kernel with our flashlight cosine similarity.\n\\[\nK_{f}(x_{i},x_{j},v_{i}) = \\exp{(1 - \\langle x_{i}, x_{j} \\rangle_{f}^{strength}})*K(x_{i},x_{j})\n\\]\n\nsource\n\nadaptive_anisotropic_kernel\n\n adaptive_anisotropic_kernel (D, k=10, alpha=1)\n\n\nsource\n\n\nanisotropic_kernel\n\n anisotropic_kernel (D, sigma=0.7, alpha=1)\n\nComputes anisotropic kernel of given distances matrix.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nD\nndarray or sparse\n\n\n\n\nsigma\nfloat\n0.7\nKernel bandwidth, by default 0.7\n\n\nalpha\nint\n1\nDegree of density normalization, from 0 to 1; by default 1\n\n\n\n\nsource\n\n\nflashlight_kernel\n\n flashlight_kernel (X, flows, kernel_type='fixed', k=10, sigma=0.7,\n                    anisotropic_density_normalization=1, flow_strength=1)\n\nA distance aware adaptation of the flashlight cosine similarity, obtained by multiplying the cosine similarity by a traditional guassian kernel. This is not intended to be differentiable, although is with some choices of kernels (anisotropic, plain).\n\nfrom FEARFRED.datasets.manifolds import directed_circle\n\n\nX, flows, labels = directed_circle(num_nodes=128)\nX = torch.tensor(X)\nflows = torch.tensor(flows)\nlabels = torch.tensor(labels)\n\n\nA = flashlight_kernel(X,flows,sigma=0.7)\nplt.imshow(A)\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\n\nA\n\ntensor([[0.0000, 0.5334, 0.5271,  ..., 0.0649, 0.0719, 0.0720],\n        [0.0723, 0.0000, 0.5633,  ..., 0.0610, 0.0674, 0.0675],\n        [0.0714, 0.0762, 0.0000,  ..., 0.0603, 0.0667, 0.0668],\n        ...,\n        [0.4761, 0.4445, 0.4391,  ..., 0.0000, 0.5124, 0.5114],\n        [0.5304, 0.4959, 0.4900,  ..., 0.0695, 0.0000, 0.2508],\n        [0.5314, 0.4968, 0.4909,  ..., 0.0694, 0.1746, 0.0000]],\n       dtype=torch.float64)\n\n\n\ntorch.max(A)\n\ntensor(0.5637, dtype=torch.float64)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FEARFRED",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "FEARFRED",
    "section": "Install",
    "text": "Install\npip install FEARFRED"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "FEARFRED",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "toy_graph_datasets.html",
    "href": "toy_graph_datasets.html",
    "title": "Toy Graph Datasets for FRED",
    "section": "",
    "text": "FRED is, at heart, an embedder of directed graphs. But what types of graphs? This notebook, and those nested under 01a01, contain a battery of toy directed graphs, including:\nThis notebook houses a sampling of these graphs, with details on how to call them and visualize them."
  },
  {
    "objectID": "toy_graph_datasets.html#graph-visualization-tools",
    "href": "toy_graph_datasets.html#graph-visualization-tools",
    "title": "Toy Graph Datasets for FRED",
    "section": "Graph Visualization Tools",
    "text": "Graph Visualization Tools\n\nsource\n\nvisualize_graph\n\n visualize_graph (data, is_networkx=False, to_undirected=False, ax=None)\n\n\nsource\n\n\nvisualize_heatmap\n\n visualize_heatmap (edge_index, order_ind=None, cmap='copper', ax=None)\n\n\nsource\n\n\ndisplay_gallery\n\n display_gallery (vizset, ncol=4)\n\n\nsource\n\n\ndisplay_graph_gallery\n\n display_graph_gallery (dataset, ncol=4)\n\n\nsource\n\n\ndisplay_heatmap_gallery\n\n display_heatmap_gallery (dataset, ncol=4)"
  },
  {
    "objectID": "toy_graph_datasets.html#add-source-and-sink-communities",
    "href": "toy_graph_datasets.html#add-source-and-sink-communities",
    "title": "Toy Graph Datasets for FRED",
    "section": "Add source and sink communities",
    "text": "Add source and sink communities\nIn a directed graph, a node is a source if it has in-degree of 0, and a node is a sink if it has out-degree of 0. These notions can be generalized into source communities and sink communities (by communities we simply mean a group of nodes in the directed graph). If we think of information on the directed graph as some mass diffusing through random walk, then mass will stay in the sink communities once enter and mass will stay out of the source communities once exit. Then, the existence of source and/or sink communities could imbalancedly affect the measurement of diffusion curvature.\nIn order to assess and possibly correct this effect, we first want to create directed graphs with source and/or sink communities. We will do so by create pretransformation to existing directed graph data so to make all edges connected to some ground-truth communities pointing outwards (for source communities) or inwards (for sink communities).\nWe will be flexible and add a parameter adv_prob to indicate the probability of edges pointing in the wrong direction. This probability should be zero or close to zero.\nThere are two possible way to correct the edges that point in the wrong direction: either by reversing the direction or by removing the edge completely. The probability of employing these methods is specified by the probability remove_prob. This parameter could help mitigate the effect of excessive number of edges pointing out of (into resp.) source (sink resp.) communities.\n\nsource\n\nSourceSink\n\n SourceSink (prob_source=0.1, prob_sink=0.1, adv_prob=0, remove_prob=0)\n\nTransform a (directed or undirected) graph into a directed graph with a proportion of the nodes with mostly out-edges and a porportion of the nodes with mostly in-edges\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprob_source\nfloat\n0.1\nmust be between 0 and 1Proportion of nodes/communities to turn into source nodes/communities(with mostly out-edges)\n\n\nprob_sink\nfloat\n0.1\nmust be between 0 and 1prob_source and prob_sink must add up to no more than 1Proportion of nodes/communities to turn into sink nodes/communities(with mostly in-edges)\n\n\nadv_prob\nint\n0\nmust be between 0 and 1Probability of in-edges for source nodes and/or out-edges for sink nodes\n\n\nremove_prob\nint\n0\nmust be between 0 and 1Probability of removing an in-edge for source nodes and/or out-edges for sink nodes1 - remove_prob is the probability of reversing the direction of in-edge for source nodes and/or out-edges for sink nodes\n\n\n\nIt is also possible to create source and sink nodes using the same SourceSink transformation to generate small directed graph with specific characteristic. We will try transforming the graph above into a big cluster of source and a smaller cluster of sinks\n\npre_transform = SourceSink(prob_source=0.6, prob_sink=0.4, adv_prob=0.1, remove_prob=0)\ndataset = SmallRandom(num_nodes=20, prob_edge=0.2, pre_transform=pre_transform)\ndata = dataset[0]\n\n/var/folders/y5/4qfj8yjj1hb97n6q01_dg0km0000gn/T/ipykernel_88073/2928479379.py:88: UserWarning: Data has no ground-truth labels\n  warnings.warn(\"Data has no ground-truth labels\")\n\n\n\nvisualize_graph(data)\n\n\n\n\n\norder_ind = torch.argsort(data.y[:,1])\nvisualize_heatmap(data.edge_index, order_ind)"
  }
]